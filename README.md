---
title: Doc Gpt Backend
emoji: ðŸ¦€
colorFrom: blue
colorTo: indigo
sdk: docker
pinned: false
app_port: 7860
---
# RAG Document Q&A Application

A production-ready Retrieval-Augmented Generation (RAG) system with strict anti-hallucination guardrails.

## ðŸŽ¯ Features

### Backend (FastAPI)
- âœ… **Modular RAG Architecture**
  - PDF text extraction (page-wise)
  - Text chunking with overlap (500 chars, 100 overlap)
  - Sentence Transformer embeddings (all-MiniLM-L6-v2)
  - FAISS vector database (local)
  - Context-only Q&A with LLM

- âœ… **Anti-Hallucination Guardrails**
  - **Similarity Threshold**: 0.75 minimum score
  - **Explicit "No Data" Response**: If relevance < threshold
  - **Context-Only Prompting**: LLM forbidden from using external knowledge
  - **Confidence Scoring**: Returns similarity scores
  - **Source Attribution**: Shows which chunks were used

### Frontend (React + Vite)
- âœ… **Modern Animated UI**
  - Framer Motion animations
  - Glassmorphism design
  - Drag & drop PDF upload
  - Animated progress stages
  - Chat-style Q&A interface

- âœ… **UX Features**
  - Progress visualization (uploading â†’ chunking â†’ embedding â†’ indexing)
  - Real-time confidence scores
  - Source citations with page numbers
  - Loading animations
  - Anti-hallucination notice

## ðŸš€ Quick Start

### Backend Setup

```bash
cd backend
pip install -r requirements.txt

# Optional: Set OpenAI API key for better answers
export OPENAI_API_KEY="your-key-here"  # Linux/Mac
set OPENAI_API_KEY=your-key-here       # Windows

# Run server
python main.py
# Server runs on http://localhost:8000
```

### Frontend Setup

```bash
cd frontend
npm install
npm run dev
# App runs on http://localhost:5173
```

## ðŸ“ Project Structure

```
rag-app/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py          # PDF text extraction
â”‚   â”‚   â”œâ”€â”€ chunker.py         # Text chunking with overlap
â”‚   â”‚   â”œâ”€â”€ embedder.py        # Sentence Transformer embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store.py    # FAISS vector database
â”‚   â”‚   â””â”€â”€ qa.py              # Q&A with anti-hallucination
â”‚   â”œâ”€â”€ data/                  # Generated files
â”‚   â”‚   â”œâ”€â”€ vectors.index      # FAISS index
â”‚   â”‚   â”œâ”€â”€ chunks.json        # Chunk metadata
â”‚   â”‚   â””â”€â”€ uploaded_document.pdf
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”œâ”€â”€ Upload.tsx     # Drag & drop upload
    â”‚   â”‚   â””â”€â”€ Chat.tsx       # Q&A interface
    â”‚   â”œâ”€â”€ services/
    â”‚   â”‚   â””â”€â”€ api.ts         # Backend API client
    â”‚   â”œâ”€â”€ App.tsx            # Main app
    â”‚   â”œâ”€â”€ main.tsx
    â”‚   â””â”€â”€ index.css          # Tailwind + custom styles
    â”œâ”€â”€ index.html
    â”œâ”€â”€ package.json
    â””â”€â”€ tailwind.config.js
```

## ðŸ§  RAG Flow

### 1. Document Upload
```
PDF â†’ Extract Text (page-wise) â†’ Chunk (500/100) â†’ Embed â†’ Store in FAISS
```

### 2. Question Answering
```
Question â†’ Embed â†’ Search FAISS â†’ Apply Threshold â†’ Generate Answer
                                         â†“
                                    < 0.75 score?
                                         â†“
                        Yes â†’ "No relevant data found"
                        No  â†’ Answer from context ONLY
```

## ðŸ›¡ï¸ Anti-Hallucination Guardrails

### 1. **Similarity Threshold Check**
```python
if top_score < 0.75:
    return "No relevant information found in the uploaded document."
```

### 2. **Context-Only System Prompt**
```python
system_prompt = """
You are a document Q&A assistant. Your ONLY job is to answer 
questions based STRICTLY on the provided context.

CRITICAL RULES:
1. ONLY use information from the provided context
2. NEVER use external knowledge or training data
3. If context doesn't contain the answer, say so explicitly
"""
```

### 3. **Explicit Messaging**
- User sees "No relevant data found" instead of hallucinated answer
- Confidence scores visible in UI
- Source chunks shown with page numbers

## ðŸŽ¨ UI Features

### Animations (Framer Motion)
- **Upload Screen**: Fade-in, drag hover effects
- **Progress Bar**: Shimmer animation, stage transitions
- **Chat Messages**: Slide-up entrance animations
- **Loading States**: Pulsing dots, rotating spinners

### Design System
- **Glassmorphism**: Semi-transparent cards with backdrop blur
- **Gradient Text**: Primary color gradients
- **Dark Mode**: Sophisticated dark palette
- **Custom Scrollbars**: Styled to match theme

## ðŸ”§ Configuration

### Backend
- **Chunk Size**: 500 characters (adjustable in `chunker.py`)
- **Overlap**: 100 characters
- **Threshold**: 0.75 similarity (adjustable in `main.py`)
- **Embedding Model**: all-MiniLM-L6-v2 (can switch to OpenAI)
- **LLM**: OpenAI GPT-3.5 (optional, falls back to simple concatenation)

### Frontend
- **API Base URL**: `http://localhost:8000` (in `services/api.ts`)
- **Colors**: Configured in `tailwind.config.js`
- **Animations**: Configured via Framer Motion props

## ðŸ“Š API Endpoints

### `POST /upload`
Upload PDF and create vector index
- **Input**: PDF file (multipart/form-data)
- **Output**: `{ status, message, chunks_created, document_name }`

### `POST /ask`
Ask a question
- **Input**: `{ question: string }`
- **Output**: `{ answer, source_chunks, confidence_score, has_relevant_data }`

### `GET /status`
Check indexing status
- **Output**: `{ is_indexed, document_name, indexed_at, total_chunks }`

### `DELETE /reset`
Reset index (upload new document)

## ðŸŽ¯ Interview Talking Points

### Architecture Decisions
1. **Modular Design**: Separated concerns (loader, chunker, embedder, store, QA)
2. **Threshold-Based Retrieval**: Prevents low-quality matches
3. **Context-Only Answering**: System prompt explicitly forbids hallucination
4. **Source Attribution**: Transparency and verifiability

### Scaling Considerations
1. **Vector Store**: FAISS â†’ Pinecone/Weaviate for production
2. **Embeddings**: Sentence Transformers â†’ OpenAI for better quality
3. **Storage**: Local files â†’ Cloud storage (S3)
4. **Caching**: Add Redis for frequently asked questions

### Future Enhancements
1. Multi-document support
2. Semantic caching for faster responses
3. Query rewriting for better retrieval
4. Hybrid search (keyword + vector)
5. User feedback loop for continuous improvement

## ðŸ“ Requirements

### Backend
- Python 3.8+
- FastAPI, Uvicorn
- PyPDF2
- sentence-transformers
- faiss-cpu
- OpenAI (optional)

### Frontend
- Node.js 16+
- React 18
- TypeScript
- Vite
- Tailwind CSS
- Framer Motion
- Axios

## ðŸš€ Production Deployment

### Backend
```bash
# Use production ASGI server
pip install gunicorn
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker
```

### Frontend
```bash
npm run build
# Serve with nginx or deploy to Vercel/Netlify
```

## ðŸ“œ License

MIT

---

**Built with â¤ï¸ for interview excellence and production readiness**
