üìÑ Retrieval-Augmented Generation (RAG)
Learning Notes & Implementation Overview
### Overview

Retrieval-Augmented Generation (RAG) is a system design pattern that combines:

‚Ä¢ Information Retrieval (from user-provided documents)
‚Ä¢ Text Generation (using Large Language Models)

Instead of relying only on a model‚Äôs pre-trained knowledge, RAG retrieves relevant information from external data sources (such as PDFs) and uses that data to generate accurate, grounded responses.

Key Note
RAG reduces hallucination by grounding responses in retrieved documents, but it does not completely eliminate hallucination.

### Why RAG Is Important
Limitations of Traditional LLMs

‚Ä¢ Rely only on training data
‚Ä¢ Cannot access private or user-specific documents
‚Ä¢ May hallucinate when information is missing

Advantages of RAG Systems

‚Ä¢ Use user-uploaded data
‚Ä¢ Retrieve only relevant content
‚Ä¢ Generate responses grounded in source documents
‚Ä¢ Enable private, domain-specific Q&A

### High-Level Workflow

1. User uploads a document (PDF)
2. Document is split into chunks
3. Each chunk is converted into a vector (embedding)
4. Vectors are stored in a Vector Store
5. User asks a question
6. Question is converted into a vector
7. Vector similarity search retrieves Top-K relevant chunks
8. Retrieved chunks + question are sent to the LLM
9. LLM generates the final answer using retrieved context

### Document Chunking Strategy
Why Chunking Is Needed

LLMs have context limits. Large documents must be split to:

‚Ä¢ Improve retrieval accuracy
‚Ä¢ Preserve semantic meaning
‚Ä¢ Optimize search performance

Common Chunk Configuration
Use Case	Chunk Size	Overlap
Academic papers	200‚Äì400 chars	50‚Äì80
News articles	400‚Äì600 chars	80‚Äì100
Manuals / Books	500‚Äì800 chars	100‚Äì150
Legal documents	800‚Äì1200 chars	150‚Äì200
Recommended Default
Chunk Size  : 500‚Äì800 characters
Chunk Overlap : 80‚Äì150 characters


Note
Overlap is not a percentage.
It is a fixed number of characters repeated between adjacent chunks to preserve context.

### Chunking Example (With Overlap)
Original Text
I am Gowtham, completed my B.E CSE at SNS College of Technology with a CGPA of 8.7.
During my course of study, I developed a strong foundation in Java and web development.

Chunk 1
I am Gowtham, completed my B.E CSE at SNS College of Technology

Chunk 2 (With Overlap)
B.E CSE at SNS College of Technology with a CGPA of 8.7

Chunk 3
CGPA of 8.7. During my course of study, I developed a strong foundation in Java

### Embeddings (Critical Concept)
What Is an Embedding?

An embedding is a numerical vector representation of text that captures semantic meaning.

‚Ä¢ Similar meaning ‚Üí vectors closer together
‚Ä¢ Different meaning ‚Üí vectors farther apart

Embedding Details

‚Ä¢ Output: Array of floating-point numbers
‚Ä¢ Common dimension: 1536

Example

[0.021, -0.334, 0.891, ...]


‚ùå Important Correction
Embedding dimension ‚â† model parameters.
Model parameters are internal neural network weights.

### Vector Similarity Search

When a user asks a question:

1. Question is converted into an embedding
2. Similarity search is performed against stored vectors
3. Top-K most similar chunks are retrieved

Similarity Metrics

‚Ä¢ Cosine Similarity (most common)
‚Ä¢ Inner Product
‚Ä¢ Euclidean Distance

Example

Query Vector ‚âà [0.234]
Chunk Vector ‚âà [0.231]
‚Üí High similarity ‚Üí Relevant chunk

### Top-K Retrieval

K defines how many relevant chunks are retrieved.

Typical Values

K = 3 or 5


‚Ä¢ Higher K ‚Üí more context, more noise
‚Ä¢ Lower K ‚Üí higher precision, possible missing context

Most systems:
‚Ä¢ Retrieve Top-K chunks
‚Ä¢ Pass them to the LLM
‚Ä¢ Let the LLM synthesize the final answer

### Vector Storage Options
FAISS

‚Ä¢ Local, file-based vector search
‚Ä¢ Fast and lightweight
‚Ä¢ Ideal for prototypes

Managed Vector Databases

‚Ä¢ Pinecone
‚Ä¢ Weaviate
‚Ä¢ Qdrant

Feature	FAISS	Pinecone
Hosting	Local	Cloud
Cost	Free	Paid
Scalability	Limited	High
Setup	Simple	Managed
### PDF Processing & Persistence
PDF Extraction

‚Ä¢ Library: pypdf
‚Ä¢ Extracts raw text from uploaded PDFs

Persistence Using Pickle

pickle is used to:

‚Ä¢ Serialize Python objects
‚Ä¢ Save them to disk
‚Ä¢ Reload without recomputation

import pickle

with open("vectors.pkl", "wb") as f:
    pickle.dump(vector_store, f)

### Minimal End-to-End RAG Code Example
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,
    chunk_overlap=100
)

chunks = splitter.split_text(document_text)
embeddings = OpenAIEmbeddings()
vector_db = FAISS.from_texts(chunks, embeddings)

query = "What is Gowtham's educational background?"
docs = vector_db.similarity_search(query, k=3)

### Final Summary

‚Ä¢ RAG combines retrieval + generation
‚Ä¢ Documents are chunked and embedded
‚Ä¢ Vectors enable semantic similarity search
‚Ä¢ Retrieved chunks ground LLM responses
‚Ä¢ FAISS enables fast local search
‚Ä¢ Pickle enables persistence
‚Ä¢ Chunk size depends on use case
